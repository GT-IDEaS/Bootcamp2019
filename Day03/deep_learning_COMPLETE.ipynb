{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "5             5420    101930     1.0           0     0  ...     11   \n",
       "6             1715      6819     2.0           0     0  ...      7   \n",
       "7             1060      9711     1.0           0     0  ...      7   \n",
       "8             1780      7470     1.0           0     0  ...      7   \n",
       "9             1890      6560     2.0           0     0  ...      7   \n",
       "10            3560      9796     1.0           0     0  ...      8   \n",
       "11            1160      6000     1.0           0     0  ...      7   \n",
       "12            1430     19901     1.5           0     0  ...      7   \n",
       "13            1370      9680     1.0           0     0  ...      7   \n",
       "14            1810      4850     1.5           0     0  ...      7   \n",
       "15            2950      5000     2.0           0     3  ...      9   \n",
       "16            1890     14040     2.0           0     0  ...      7   \n",
       "17            1600      4300     1.5           0     0  ...      7   \n",
       "18            1200      9850     1.0           0     0  ...      7   \n",
       "19            1250      9774     1.0           0     0  ...      7   \n",
       "20            1620      4980     1.0           0     0  ...      7   \n",
       "21            3050     44867     1.0           0     4  ...      9   \n",
       "22            2270      6300     2.0           0     0  ...      8   \n",
       "23            1070      9643     1.0           0     0  ...      7   \n",
       "24            2450      6500     2.0           0     0  ...      8   \n",
       "25            1710      4697     1.5           0     0  ...      6   \n",
       "26            2450      2691     2.0           0     0  ...      8   \n",
       "27            1400      1581     1.5           0     0  ...      8   \n",
       "28            1520      6380     1.0           0     0  ...      7   \n",
       "29            2570      7173     2.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21583          710      1157     2.0           0     0  ...      7   \n",
       "21584         1260       900     2.0           0     0  ...      7   \n",
       "21585         1870      5000     2.0           0     0  ...      7   \n",
       "21586         1430      1201     3.0           0     0  ...      8   \n",
       "21587         1520      1488     3.0           0     0  ...      8   \n",
       "21588         1210      1278     2.0           0     0  ...      8   \n",
       "21589         2540      4760     2.0           0     0  ...      8   \n",
       "21590         4910      9444     1.5           0     0  ...     11   \n",
       "21591         2770      3852     2.0           0     0  ...      8   \n",
       "21592         1190      1200     3.0           0     0  ...      8   \n",
       "21593         4170      8142     2.0           0     2  ...     10   \n",
       "21594         2500      5995     2.0           0     0  ...      8   \n",
       "21595         1530       981     3.0           0     0  ...      8   \n",
       "21596         3600      9437     2.0           0     0  ...      9   \n",
       "21597         3410     10125     2.0           0     0  ...     10   \n",
       "21598         3118      7866     2.0           0     2  ...      9   \n",
       "21599         3990      7838     2.0           0     0  ...      9   \n",
       "21600         4470      8088     2.0           0     0  ...     11   \n",
       "21601         1425      1179     3.0           0     0  ...      8   \n",
       "21602         1500     11968     1.0           0     0  ...      6   \n",
       "21603         2270      5536     2.0           0     0  ...      8   \n",
       "21604         1490      1126     3.0           0     0  ...      8   \n",
       "21605         2520      6023     2.0           0     0  ...      9   \n",
       "21606         3510      7200     2.0           0     0  ...      9   \n",
       "21607         1310      1294     2.0           0     0  ...      8   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 2003, 1942,\n",
       "       1927, 1977, 1900, 1979, 1994, 1916, 1921, 1969, 1947, 1968, 1985,\n",
       "       1941, 1915, 1909, 1948, 2005, 1929, 1981, 1930, 1904, 1996, 2000,\n",
       "       1984, 2014, 1922, 1959, 1966, 1953, 1950, 2008, 1991, 1954, 1973,\n",
       "       1925, 1989, 1972, 1986, 1956, 2002, 1992, 1964, 1952, 1961, 2006,\n",
       "       1988, 1962, 1939, 1946, 1967, 1975, 1980, 1910, 1983, 1978, 1905,\n",
       "       1971, 2010, 1945, 1924, 1990, 1914, 1926, 2004, 1923, 2007, 1976,\n",
       "       1949, 1999, 1901, 1993, 1920, 1997, 1943, 1957, 1940, 1918, 1928,\n",
       "       1974, 1911, 1936, 1937, 1982, 1908, 1931, 1998, 1913, 2013, 1907,\n",
       "       1958, 2012, 1912, 2011, 1917, 1932, 1944, 1902, 2009, 1903, 1970,\n",
       "       2015, 1934, 1938, 1919, 1906, 1935])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"yr_built\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 362423227672.1664\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 241266003673.0880\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 104657655149.3632\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 76096611719.5776\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 74086634251.8784\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 72355529228.2880\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 70839192594.0224\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 69601997766.6560\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 68416932983.6032\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 67236469722.3168\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 66372671596.1344\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 65672606646.2720\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 65128629390.5408\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64582580106.0352\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64184697303.8592\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63889603126.8864\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63805269973.4016\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63460876900.7616\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63324490366.9760\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63048134321.7664\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62751331519.6928\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62455936004.9152\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62417944589.1072\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62189422287.2576\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61865934422.0160\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61621737881.6000\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61332560871.4240\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 60933667232.1536\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 60915383461.4784\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 60524351337.2672\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 60173872372.1216\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 59914683246.1824\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59413763280.0768\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 59187383107.5840\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 58804822979.3792\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 58427143749.6320\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 58193626582.2208\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 0s 15us/step - loss: 57860970958.0288\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 57484226763.1616\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 57240758937.1904\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 0s 12us/step - loss: 57191458681.6512\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 56880555216.0768\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 56534853576.2944\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 0s 11us/step - loss: 56118989907.5584\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 56056098573.5168\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 56078933458.9440\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 56090581958.6560\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 55637524047.4624\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 0s 11us/step - loss: 55694483128.3200\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 55447747251.4048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train,\n",
    "                        epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49127404144950154"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "             activation='relu',\n",
    "             layers=[20,20],\n",
    "             loss='mean_squared_error'):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(50, input_dim=input_len, activation=activ))\n",
    "    model.add(keras.layers.Dense(50, activation=activ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532974.1758 - val_loss: 557914.6710\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 532967.3181 - val_loss: 557908.9735\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532962.1269 - val_loss: 557903.9525\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532957.2894 - val_loss: 557899.4930\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532952.5633 - val_loss: 557894.6470\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532947.9042 - val_loss: 557889.8510\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532943.2878 - val_loss: 557885.5170\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532938.6823 - val_loss: 557880.8320\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532934.1124 - val_loss: 557876.0225\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532929.5258 - val_loss: 557871.6970\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532924.9893 - val_loss: 557867.0405\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532920.4284 - val_loss: 557862.6110\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532915.8835 - val_loss: 557857.9335\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532911.3458 - val_loss: 557853.6650\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532906.8151 - val_loss: 557848.8690\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532902.2797 - val_loss: 557844.4835\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532897.7445 - val_loss: 557839.8085\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532893.2282 - val_loss: 557835.5010\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532888.7034 - val_loss: 557830.8360\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532884.1843 - val_loss: 557826.4430\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532879.6504 - val_loss: 557821.7920\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532875.1483 - val_loss: 557817.4790\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532870.6122 - val_loss: 557812.8160\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532866.1056 - val_loss: 557808.3360\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532861.5706 - val_loss: 557803.7380\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532857.0776 - val_loss: 557799.1610\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532852.5495 - val_loss: 557794.6775\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532848.0404 - val_loss: 557789.9945\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 532843.5083 - val_loss: 557785.6990\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532839.0229 - val_loss: 557781.0485\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532834.4841 - val_loss: 557776.6410\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532829.9850 - val_loss: 557771.9805\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532825.4563 - val_loss: 557767.6930\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532820.7999 - val_loss: 557762.6110\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532815.6766 - val_loss: 557757.6930\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532810.8327 - val_loss: 557752.8360\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532806.0533 - val_loss: 557747.9525\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532801.3039 - val_loss: 557743.0485\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532796.0384 - val_loss: 557737.7920\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532790.9252 - val_loss: 557732.8320\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532785.9397 - val_loss: 557727.7980\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532781.0243 - val_loss: 557722.8730\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 532776.1177 - val_loss: 557717.9525\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532771.2528 - val_loss: 557713.3645\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532766.3882 - val_loss: 557708.4470\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532761.5342 - val_loss: 557703.6650\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532756.7074 - val_loss: 557698.6670\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532751.8659 - val_loss: 557693.7030\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532746.5045 - val_loss: 557688.0105\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532741.2496 - val_loss: 557683.0085\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532968.8535 - val_loss: 557907.8785\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532959.6811 - val_loss: 557900.1345\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532952.1103 - val_loss: 557892.8225\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532944.7126 - val_loss: 557885.5415\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532937.3764 - val_loss: 557877.9805\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532930.0783 - val_loss: 557870.8360\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532922.8012 - val_loss: 557863.6790\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532915.5595 - val_loss: 557856.4430\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532908.3144 - val_loss: 557849.0125\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532901.0774 - val_loss: 557841.7920\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532893.8619 - val_loss: 557834.6450\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532886.6362 - val_loss: 557827.5335\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532879.4228 - val_loss: 557820.0150\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532872.2118 - val_loss: 557812.9090\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532864.9957 - val_loss: 557805.7820\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532857.7976 - val_loss: 557798.6210\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 532850.5966 - val_loss: 557791.5170\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532843.3934 - val_loss: 557783.9945\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 532836.1900 - val_loss: 557776.8790\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 532828.9844 - val_loss: 557769.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532821.7987 - val_loss: 557762.6210\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532814.5922 - val_loss: 557755.5335\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532807.4018 - val_loss: 557747.9945\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532800.2038 - val_loss: 557740.9595\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532793.0068 - val_loss: 557733.7920\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532785.8116 - val_loss: 557726.6450\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532778.6256 - val_loss: 557719.5335\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532771.4330 - val_loss: 557712.0170\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532764.2351 - val_loss: 557704.9855\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532757.0397 - val_loss: 557697.7920\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532749.8561 - val_loss: 557690.6450\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532742.6587 - val_loss: 557683.5700\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532735.4679 - val_loss: 557676.3360\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532728.2726 - val_loss: 557668.9955\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 532721.0774 - val_loss: 557661.7980\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532713.8920 - val_loss: 557654.6670\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532706.6927 - val_loss: 557647.6650\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532699.5059 - val_loss: 557640.4430\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532692.3229 - val_loss: 557633.0485\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532685.1196 - val_loss: 557625.8125\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532677.9398 - val_loss: 557618.7840\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532670.7418 - val_loss: 557611.6650\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532663.5602 - val_loss: 557604.4430\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532656.3681 - val_loss: 557597.0485\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532649.1709 - val_loss: 557589.8230\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532641.9852 - val_loss: 557582.8240\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 532634.7951 - val_loss: 557575.6850\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532627.6101 - val_loss: 557568.4755\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532620.4128 - val_loss: 557561.1510\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532613.2224 - val_loss: 557553.9335\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 460302.7814 - val_loss: 389935.7043\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 288842.3789 - val_loss: 218617.1030\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 174847.2772 - val_loss: 176664.5083\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 165716.2985 - val_loss: 173251.2797\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 163249.9868 - val_loss: 172159.8714\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 161611.9619 - val_loss: 170750.6835\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 160407.1660 - val_loss: 169810.5179\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 159345.5089 - val_loss: 169291.1629\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 158630.9237 - val_loss: 169034.1908\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 158230.8990 - val_loss: 168649.8141\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 158015.7605 - val_loss: 169410.4244\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 157256.5905 - val_loss: 167973.3121\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 156975.5700 - val_loss: 167988.3602\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 156917.9833 - val_loss: 167261.6631\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 156874.7785 - val_loss: 169002.1360\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156387.4673 - val_loss: 167151.6112\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156141.9905 - val_loss: 168106.7525\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155940.2835 - val_loss: 167468.5219\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155684.3832 - val_loss: 167765.7778\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155719.0916 - val_loss: 167466.6630\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155545.2087 - val_loss: 166713.0641\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 155303.3075 - val_loss: 166852.0119\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 155179.0475 - val_loss: 166528.6830\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155039.3024 - val_loss: 167060.9409\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155022.6373 - val_loss: 166782.4909\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154666.9540 - val_loss: 166420.0279\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155009.4956 - val_loss: 166587.0267\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154355.8122 - val_loss: 166219.7179\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154413.5130 - val_loss: 166502.0960\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154332.8766 - val_loss: 165990.8961\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154091.2003 - val_loss: 167065.7801\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 154115.9829 - val_loss: 167038.7180\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153895.3677 - val_loss: 166218.6487\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154062.3794 - val_loss: 166869.0389\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153830.0327 - val_loss: 165967.7117\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153554.7361 - val_loss: 165572.1533\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153472.7553 - val_loss: 165612.7610\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153322.2003 - val_loss: 165850.4700\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153419.3344 - val_loss: 166075.8007\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 153015.61 - 0s 10us/step - loss: 153535.5968 - val_loss: 165214.1501\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153215.4091 - val_loss: 165272.4703\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153143.2716 - val_loss: 165429.7129\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152930.2451 - val_loss: 164992.4201\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 153128.0189 - val_loss: 165270.4543\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 152793.8060 - val_loss: 166957.1629\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 152620.9163 - val_loss: 166043.3472\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 152873.9664 - val_loss: 165305.3497\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152703.1435 - val_loss: 165293.5600\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 152441.4831 - val_loss: 164842.7197\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 152284.0022 - val_loss: 164795.3660\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.6314101076795888\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 # bad\n",
    "\n",
    "# loop over chosen activation functions, train, evaluate on validation\n",
    "for activ in ['sigmoid', 'tanh', 'relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.158106372019069e-17\n",
      "1.0\n",
      "0.016119707489855875\n",
      "0.9970087526331801\n",
      "0.09330835574898895\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 416644724774.2293 - val_loss: 456240251273.2160\n",
      "Epoch 2/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416219411047.3102 - val_loss: 455230525276.1600\n",
      "Epoch 3/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 414539373463.3245 - val_loss: 452292457267.2000\n",
      "Epoch 4/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 410731792826.3680 - val_loss: 446517339226.1120\n",
      "Epoch 5/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 403953550943.1182 - val_loss: 437057174896.6400\n",
      "Epoch 6/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 393657770544.6969 - val_loss: 423509280423.9360\n",
      "Epoch 7/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 379580761156.2667 - val_loss: 405581878001.6640\n",
      "Epoch 8/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 361658841912.6613 - val_loss: 383390637883.3920\n",
      "Epoch 9/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 340165479570.5458 - val_loss: 357764951965.6960\n",
      "Epoch 10/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 315699842136.2916 - val_loss: 329091613720.5760\n",
      "Epoch 11/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 289117304433.3227 - val_loss: 298578878136.3200\n",
      "Epoch 12/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 261380564058.1120 - val_loss: 267361173110.7840\n",
      "Epoch 13/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 233512805422.4213 - val_loss: 236796970795.0080\n",
      "Epoch 14/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 206621634811.2213 - val_loss: 207759685582.8480\n",
      "Epoch 15/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 181544216108.1458 - val_loss: 181255742947.3280\n",
      "Epoch 16/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 159017826516.9920 - val_loss: 158054828081.1520\n",
      "Epoch 17/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 139581889431.3244 - val_loss: 138607148466.1760\n",
      "Epoch 18/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 123407551306.8658 - val_loss: 122830300577.7920\n",
      "Epoch 19/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 110414201050.4533 - val_loss: 110407385219.0720\n",
      "Epoch 20/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 100342549796.1813 - val_loss: 101138505334.7840\n",
      "Epoch 21/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 92746195562.9511 - val_loss: 94260454621.1840\n",
      "Epoch 22/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 87142331467.5484 - val_loss: 89257073442.8160\n",
      "Epoch 23/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 83029246782.1227 - val_loss: 85649517051.9040\n",
      "Epoch 24/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 79974571289.2587 - val_loss: 82916333518.8480\n",
      "Epoch 25/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 77613710038.3573 - val_loss: 80772310892.5440\n",
      "Epoch 26/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 75758766536.9315 - val_loss: 79040771063.8080\n",
      "Epoch 27/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 74215229277.9805 - val_loss: 77595545501.6960\n",
      "Epoch 28/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 72905572177.2373 - val_loss: 76338272731.1360\n",
      "Epoch 29/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 71745211261.8382 - val_loss: 75183492169.7280\n",
      "Epoch 30/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 70697890440.0782 - val_loss: 74151740702.7200\n",
      "Epoch 31/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 69729018285.6249 - val_loss: 73179619459.0720\n",
      "Epoch 32/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 68816311542.6702 - val_loss: 72256881098.7520\n",
      "Epoch 33/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 67936579977.2160 - val_loss: 71377126752.2560\n",
      "Epoch 34/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 67097538195.9111 - val_loss: 70526461837.3120\n",
      "Epoch 35/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 66280199538.4604 - val_loss: 69706505519.1040\n",
      "Epoch 36/200\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 65474546091.8044 - val_loss: 68888741609.4720\n",
      "Epoch 37/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 64683688744.2773 - val_loss: 68096607354.8800\n",
      "Epoch 38/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 63904897171.4560 - val_loss: 67312825925.6320\n",
      "Epoch 39/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 63130683200.8533 - val_loss: 66542651277.3120\n",
      "Epoch 40/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 62353993003.4631 - val_loss: 65772004278.2720\n",
      "Epoch 41/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61587946799.1040 - val_loss: 65005461667.8400\n",
      "Epoch 42/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 60821804999.5662 - val_loss: 64246054682.6240\n",
      "Epoch 43/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 60059479146.4960 - val_loss: 63493657821.1840\n",
      "Epoch 44/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 59302580489.7849 - val_loss: 62744275124.2240\n",
      "Epoch 45/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 58543664973.5964 - val_loss: 61995190353.9200\n",
      "Epoch 46/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 57782123597.3689 - val_loss: 61241564889.0880\n",
      "Epoch 47/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 57033753307.8187 - val_loss: 60478097522.6880\n",
      "Epoch 48/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 56258215957.8453 - val_loss: 59734967582.7200\n",
      "Epoch 49/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 55499806911.1467 - val_loss: 58974680743.9360\n",
      "Epoch 50/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 54732866202.2827 - val_loss: 58233620856.8320\n",
      "Epoch 51/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 53979515737.4293 - val_loss: 57494574628.8640\n",
      "Epoch 52/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 53222918094.8480 - val_loss: 56758398943.2320\n",
      "Epoch 53/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 52461172330.0409 - val_loss: 56031989923.8400\n",
      "Epoch 54/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51707750733.1413 - val_loss: 55297092943.8720\n",
      "Epoch 55/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50952434705.2942 - val_loss: 54574977581.0560\n",
      "Epoch 56/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50216374794.4676 - val_loss: 53872021700.6080\n",
      "Epoch 57/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 49466069960.4764 - val_loss: 53165771784.1920\n",
      "Epoch 58/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 48736365170.2329 - val_loss: 52481645215.7440\n",
      "Epoch 59/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 48015811634.0622 - val_loss: 51818096754.6880\n",
      "Epoch 60/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 47307462479.4169 - val_loss: 51166277468.1600\n",
      "Epoch 61/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 46607229739.9182 - val_loss: 50528075153.4080\n",
      "Epoch 62/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 45922461060.6649 - val_loss: 49895826849.7920\n",
      "Epoch 63/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45252787397.5182 - val_loss: 49297462722.5600\n",
      "Epoch 64/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44604951643.9324 - val_loss: 48700301737.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 43967003383.1253 - val_loss: 48120822792.1920\n",
      "Epoch 66/200\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 43677351207.822 - 0s 10us/step - loss: 43358731547.0791 - val_loss: 47560437465.0880\n",
      "Epoch 67/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42763919989.8738 - val_loss: 47031392337.9200\n",
      "Epoch 68/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 42186531049.0169 - val_loss: 46524311076.8640\n",
      "Epoch 69/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 41641749632.3413 - val_loss: 46035936804.8640\n",
      "Epoch 70/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 41131274274.5884 - val_loss: 45590292496.3840\n",
      "Epoch 71/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40640540990.5778 - val_loss: 45200045047.8080\n",
      "Epoch 72/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40185976102.0018 - val_loss: 44812461441.0240\n",
      "Epoch 73/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39770515632.5831 - val_loss: 44455844544.5120\n",
      "Epoch 74/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39383302529.9342 - val_loss: 44143539847.1680\n",
      "Epoch 75/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39036987379.2569 - val_loss: 43853232668.6720\n",
      "Epoch 76/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38714754087.1396 - val_loss: 43603367395.3280\n",
      "Epoch 77/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38427186877.7813 - val_loss: 43368991490.0480\n",
      "Epoch 78/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 38160408874.5529 - val_loss: 43161227362.3040\n",
      "Epoch 79/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37924018501.8596 - val_loss: 42971564965.8880\n",
      "Epoch 80/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37706286556.0462 - val_loss: 42789223038.9760\n",
      "Epoch 81/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 37521690509.3120 - val_loss: 42630109265.9200\n",
      "Epoch 82/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37323107128.6613 - val_loss: 42519357882.3680\n",
      "Epoch 83/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 37151890132.5369 - val_loss: 42389031780.3520\n",
      "Epoch 84/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 36994491953.6071 - val_loss: 42256742350.8480\n",
      "Epoch 85/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36835559378.4889 - val_loss: 42120880259.0720\n",
      "Epoch 86/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36704747214.1653 - val_loss: 42013906468.8640\n",
      "Epoch 87/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36570395162.8516 - val_loss: 41910531948.5440\n",
      "Epoch 88/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36440781664.7111 - val_loss: 41807704260.6080\n",
      "Epoch 89/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36317897958.2862 - val_loss: 41708020006.9120\n",
      "Epoch 90/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36208089410.2187 - val_loss: 41619933167.6160\n",
      "Epoch 91/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36095894862.0516 - val_loss: 41511169458.1760\n",
      "Epoch 92/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35994212051.6267 - val_loss: 41430843686.9120\n",
      "Epoch 93/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35886885553.9484 - val_loss: 41361790697.4720\n",
      "Epoch 94/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35793174910.2933 - val_loss: 41256490467.3280\n",
      "Epoch 95/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35695162993.3227 - val_loss: 41184487899.1360\n",
      "Epoch 96/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35602815862.5564 - val_loss: 41073298571.2640\n",
      "Epoch 97/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35513192074.8089 - val_loss: 40987678146.5600\n",
      "Epoch 98/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35420576619.6338 - val_loss: 40920484282.3680\n",
      "Epoch 99/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35349878811.3067 - val_loss: 40858422837.2480\n",
      "Epoch 100/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35263009479.7938 - val_loss: 40782295302.1440\n",
      "Epoch 101/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35188018318.9049 - val_loss: 40695475666.9440\n",
      "Epoch 102/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35094649547.4347 - val_loss: 40589422854.1440\n",
      "Epoch 103/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35030873797.0631 - val_loss: 40532539834.3680\n",
      "Epoch 104/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34941361824.6542 - val_loss: 40445707354.1120\n",
      "Epoch 105/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34875362002.7164 - val_loss: 40372683702.2720\n",
      "Epoch 106/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34801925383.0542 - val_loss: 40293627822.0800\n",
      "Epoch 107/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34740944914.2044 - val_loss: 40232055275.5200\n",
      "Epoch 108/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34671356710.4569 - val_loss: 40199075725.3120\n",
      "Epoch 109/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34603361945.3724 - val_loss: 40129020493.8240\n",
      "Epoch 110/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34540628758.9831 - val_loss: 40064682721.2800\n",
      "Epoch 111/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34478302221.6533 - val_loss: 39999905398.7840\n",
      "Epoch 112/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34424151026.3467 - val_loss: 39946603134.9760\n",
      "Epoch 113/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34350522548.2240 - val_loss: 39869459628.0320\n",
      "Epoch 114/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34293735635.1716 - val_loss: 39808236847.1040\n",
      "Epoch 115/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34235366901.5324 - val_loss: 39718557646.8480\n",
      "Epoch 116/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34184416892.2453 - val_loss: 39678572363.7760\n",
      "Epoch 117/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34140378854.7413 - val_loss: 39588100669.4400\n",
      "Epoch 118/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34081494005.9876 - val_loss: 39570870108.1600\n",
      "Epoch 119/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34034455769.5431 - val_loss: 39493161254.9120\n",
      "Epoch 120/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33986731443.9964 - val_loss: 39435902451.7120\n",
      "Epoch 121/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33928106806.8409 - val_loss: 39386770636.8000\n",
      "Epoch 122/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33890774086.0871 - val_loss: 39336803696.6400\n",
      "Epoch 123/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33837027029.4471 - val_loss: 39311237513.2160\n",
      "Epoch 124/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33790312047.5022 - val_loss: 39268650778.6240\n",
      "Epoch 125/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33744856614.6844 - val_loss: 39220975042.5600\n",
      "Epoch 126/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33708920902.9973 - val_loss: 39168369328.1280\n",
      "Epoch 127/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33664118831.3316 - val_loss: 39107614572.5440\n",
      "Epoch 128/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33618047434.7520 - val_loss: 39077930663.9360\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 13us/step - loss: 33579016670.7769 - val_loss: 39000030150.6560\n",
      "Epoch 130/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33535163312.8107 - val_loss: 38973689102.3360\n",
      "Epoch 131/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33492335205.4898 - val_loss: 38930244927.4880\n",
      "Epoch 132/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33455017304.9742 - val_loss: 38909504585.7280\n",
      "Epoch 133/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33410613950.6916 - val_loss: 38849598160.8960\n",
      "Epoch 134/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33375177029.8596 - val_loss: 38805699493.8880\n",
      "Epoch 135/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33335135123.6836 - val_loss: 38765521666.0480\n",
      "Epoch 136/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33302001097.8418 - val_loss: 38731508088.8320\n",
      "Epoch 137/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33272129013.5324 - val_loss: 38691386785.7920\n",
      "Epoch 138/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33235998062.8196 - val_loss: 38653477912.5760\n",
      "Epoch 139/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33192606025.5004 - val_loss: 38581751644.1600\n",
      "Epoch 140/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33160514983.2533 - val_loss: 38569467052.0320\n",
      "Epoch 141/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33130229130.1262 - val_loss: 38560806535.1680\n",
      "Epoch 142/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33097339705.5716 - val_loss: 38533460557.8240\n",
      "Epoch 143/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33071230875.8756 - val_loss: 38499154231.2960\n",
      "Epoch 144/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33035680088.0640 - val_loss: 38425215172.6080\n",
      "Epoch 145/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33002553438.6631 - val_loss: 38392848777.2160\n",
      "Epoch 146/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32988974998.4142 - val_loss: 38361419939.8400\n",
      "Epoch 147/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32941554934.6702 - val_loss: 38362186940.4160\n",
      "Epoch 148/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32918950730.8658 - val_loss: 38352234151.9360\n",
      "Epoch 149/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32880421586.7164 - val_loss: 38287552675.8400\n",
      "Epoch 150/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32854975978.6098 - val_loss: 38253683376.1280\n",
      "Epoch 151/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32840697178.7947 - val_loss: 38210115403.7760\n",
      "Epoch 152/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32803013911.4382 - val_loss: 38197209858.0480\n",
      "Epoch 153/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32776528858.6809 - val_loss: 38171726905.3440\n",
      "Epoch 154/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32748213445.5182 - val_loss: 38138094190.5920\n",
      "Epoch 155/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32722838243.1004 - val_loss: 38122217734.1440\n",
      "Epoch 156/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32695236014.5351 - val_loss: 38068723449.8560\n",
      "Epoch 157/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32670382154.6382 - val_loss: 38065923162.1120\n",
      "Epoch 158/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32645579341.8240 - val_loss: 37994074636.2880\n",
      "Epoch 159/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32624472003.9253 - val_loss: 37982525947.9040\n",
      "Epoch 160/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32597628986.2542 - val_loss: 37980290088.9600\n",
      "Epoch 161/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32567088693.2480 - val_loss: 37934505525.2480\n",
      "Epoch 162/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32546711400.9031 - val_loss: 37949333405.6960\n",
      "Epoch 163/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32516446144.2844 - val_loss: 37929517776.8960\n",
      "Epoch 164/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32502933589.5609 - val_loss: 37870684536.8320\n",
      "Epoch 165/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32481144615.3671 - val_loss: 37861278351.3600\n",
      "Epoch 166/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32453420915.8258 - val_loss: 37832754200.5760\n",
      "Epoch 167/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32430444946.3182 - val_loss: 37816830328.8320\n",
      "Epoch 168/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32408308675.0151 - val_loss: 37782546350.0800\n",
      "Epoch 169/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32385081331.2569 - val_loss: 37786984120.3200\n",
      "Epoch 170/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32365805653.5609 - val_loss: 37741922549.7600\n",
      "Epoch 171/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32349541994.0409 - val_loss: 37729805271.0400\n",
      "Epoch 172/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32322018574.3360 - val_loss: 37718481600.5120\n",
      "Epoch 173/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32296829794.5316 - val_loss: 37701888802.8160\n",
      "Epoch 174/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32277643309.5111 - val_loss: 37663694651.3920\n",
      "Epoch 175/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32269726715.4489 - val_loss: 37674866638.8480\n",
      "Epoch 176/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32233154066.6596 - val_loss: 37626876854.2720\n",
      "Epoch 177/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32213190499.4418 - val_loss: 37591130505.2160\n",
      "Epoch 178/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32196344607.1751 - val_loss: 37617813454.8480\n",
      "Epoch 179/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32168589903.6444 - val_loss: 37551676588.0320\n",
      "Epoch 180/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32146972701.1271 - val_loss: 37532002385.9200\n",
      "Epoch 181/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32130249115.4204 - val_loss: 37544678719.4880\n",
      "Epoch 182/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32112177372.2738 - val_loss: 37497368084.4800\n",
      "Epoch 183/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32091575828.4800 - val_loss: 37469483335.6800\n",
      "Epoch 184/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32067664347.1360 - val_loss: 37462552870.9120\n",
      "Epoch 185/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32044462150.9973 - val_loss: 37429701935.1040\n",
      "Epoch 186/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32034467963.7902 - val_loss: 37426921144.3200\n",
      "Epoch 187/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32019949296.7538 - val_loss: 37409924055.0400\n",
      "Epoch 188/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 31997113823.6871 - val_loss: 37370300858.3680\n",
      "Epoch 189/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31977750718.2364 - val_loss: 37362002427.9040\n",
      "Epoch 190/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31954116509.6960 - val_loss: 37339767603.2000\n",
      "Epoch 191/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31939885838.7911 - val_loss: 37316951769.0880\n",
      "Epoch 192/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31924599555.8684 - val_loss: 37286352683.0080\n",
      "Epoch 193/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31895322201.6569 - val_loss: 37303470850.0480\n",
      "Epoch 194/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31877078220.8000 - val_loss: 37286238158.8480\n",
      "Epoch 195/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31859688972.2880 - val_loss: 37263179186.1760\n",
      "Epoch 196/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31845225469.2693 - val_loss: 37228672647.1680\n",
      "Epoch 197/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31821257150.0089 - val_loss: 37222641401.8560\n",
      "Epoch 198/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31811658725.6036 - val_loss: 37176441765.8880\n",
      "Epoch 199/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31781443567.6160 - val_loss: 37216355942.4000\n",
      "Epoch 200/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31768730311.7938 - val_loss: 37173094449.1520\n",
      "0.6651870143377775\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=200, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 416642251358.2080 - val_loss: 456230491127.8080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456230491127.80798, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416175593648.5831 - val_loss: 455115510906.8800\n",
      "\n",
      "Epoch 00002: val_loss improved from 456230491127.80798 to 455115510906.88000, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 414236468168.4764 - val_loss: 451725259702.2720\n",
      "\n",
      "Epoch 00003: val_loss improved from 455115510906.88000 to 451725259702.27197, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 409652313427.5129 - val_loss: 444793102532.6080\n",
      "\n",
      "Epoch 00004: val_loss improved from 451725259702.27197 to 444793102532.60797, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 401502478940.3876 - val_loss: 433555929300.9920\n",
      "\n",
      "Epoch 00005: val_loss improved from 444793102532.60797 to 433555929300.99200, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 389127931691.0080 - val_loss: 417407007195.1360\n",
      "\n",
      "Epoch 00006: val_loss improved from 433555929300.99200 to 417407007195.13599, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 372354871401.5858 - val_loss: 396453596364.8000\n",
      "\n",
      "Epoch 00007: val_loss improved from 417407007195.13599 to 396453596364.79999, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 351261101246.2365 - val_loss: 370822052315.1360\n",
      "\n",
      "Epoch 00008: val_loss improved from 396453596364.79999 to 370822052315.13599, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 326316790761.2444 - val_loss: 341345543389.1840\n",
      "\n",
      "Epoch 00009: val_loss improved from 370822052315.13599 to 341345543389.18402, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 298425331511.7511 - val_loss: 309198200242.1760\n",
      "\n",
      "Epoch 00010: val_loss improved from 341345543389.18402 to 309198200242.17603, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 268705647005.2409 - val_loss: 275747260858.3680\n",
      "\n",
      "Epoch 00011: val_loss improved from 309198200242.17603 to 275747260858.36798, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 238421556796.5298 - val_loss: 242600971730.9440\n",
      "\n",
      "Epoch 00012: val_loss improved from 275747260858.36798 to 242600971730.94400, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 208827929067.5200 - val_loss: 210544567123.9680\n",
      "\n",
      "Epoch 00013: val_loss improved from 242600971730.94400 to 210544567123.96799, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 181250766609.5218 - val_loss: 181239822417.9200\n",
      "\n",
      "Epoch 00014: val_loss improved from 210544567123.96799 to 181239822417.92001, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 156734730928.1280 - val_loss: 155991138369.5360\n",
      "\n",
      "Epoch 00015: val_loss improved from 181239822417.92001 to 155991138369.53601, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 138017311735.09 - 0s 11us/step - loss: 135798981525.5040 - val_loss: 134988053086.2080\n",
      "\n",
      "Epoch 00016: val_loss improved from 155991138369.53601 to 134988053086.20799, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 118836078000.3556 - val_loss: 118353070260.2240\n",
      "\n",
      "Epoch 00017: val_loss improved from 134988053086.20799 to 118353070260.22400, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 105603678346.3538 - val_loss: 105883129348.0960\n",
      "\n",
      "Epoch 00018: val_loss improved from 118353070260.22400 to 105883129348.09599, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 95745313909.4187 - val_loss: 96829833609.2160\n",
      "\n",
      "Epoch 00019: val_loss improved from 105883129348.09599 to 96829833609.21600, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 88618352621.7956 - val_loss: 90346775052.2880\n",
      "\n",
      "Epoch 00020: val_loss improved from 96829833609.21600 to 90346775052.28799, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 83567574355.5129 - val_loss: 85874045485.0560\n",
      "\n",
      "Epoch 00021: val_loss improved from 90346775052.28799 to 85874045485.05600, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 79998094813.8667 - val_loss: 82761349857.2800\n",
      "\n",
      "Epoch 00022: val_loss improved from 85874045485.05600 to 82761349857.28000, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 77425343874.8444 - val_loss: 80483269541.8880\n",
      "\n",
      "Epoch 00023: val_loss improved from 82761349857.28000 to 80483269541.88800, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 75462087511.6089 - val_loss: 78729282191.3600\n",
      "\n",
      "Epoch 00024: val_loss improved from 80483269541.88800 to 78729282191.36000, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 73870388468.8498 - val_loss: 77273078038.5280\n",
      "\n",
      "Epoch 00025: val_loss improved from 78729282191.36000 to 77273078038.52800, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 72516323071.3173 - val_loss: 75994380828.6720\n",
      "\n",
      "Epoch 00026: val_loss improved from 77273078038.52800 to 75994380828.67200, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 71323900251.7049 - val_loss: 74852082581.5040\n",
      "\n",
      "Epoch 00027: val_loss improved from 75994380828.67200 to 74852082581.50400, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 70227146804.7929 - val_loss: 73817158909.9520\n",
      "\n",
      "Epoch 00028: val_loss improved from 74852082581.50400 to 73817158909.95200, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 69211586836.7076 - val_loss: 72820976910.3360\n",
      "\n",
      "Epoch 00029: val_loss improved from 73817158909.95200 to 72820976910.33600, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 68253316495.5876 - val_loss: 71868724150.2720\n",
      "\n",
      "Epoch 00030: val_loss improved from 72820976910.33600 to 71868724150.27200, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 67342146957.7671 - val_loss: 70966434824.1920\n",
      "\n",
      "Epoch 00031: val_loss improved from 71868724150.27200 to 70966434824.19200, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 66443596828.2169 - val_loss: 70082538242.0480\n",
      "\n",
      "Epoch 00032: val_loss improved from 70966434824.19200 to 70082538242.04800, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 65572032122.4249 - val_loss: 69227643109.3760\n",
      "\n",
      "Epoch 00033: val_loss improved from 70082538242.04800 to 69227643109.37601, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 64735541992.5618 - val_loss: 68379734900.7360\n",
      "\n",
      "Epoch 00034: val_loss improved from 69227643109.37601 to 68379734900.73600, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63888829455.4738 - val_loss: 67542714089.4720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 68379734900.73600 to 67542714089.47200, saving model to best_model.h5\n",
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63058047835.2498 - val_loss: 66706866110.4640\n",
      "\n",
      "Epoch 00036: val_loss improved from 67542714089.47200 to 66706866110.46400, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62235788523.7476 - val_loss: 65874906316.8000\n",
      "\n",
      "Epoch 00037: val_loss improved from 66706866110.46400 to 65874906316.80000, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 61422055413.0773 - val_loss: 65048028676.0960\n",
      "\n",
      "Epoch 00038: val_loss improved from 65874906316.80000 to 65048028676.09600, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 60599847682.0480 - val_loss: 64214742892.5440\n",
      "\n",
      "Epoch 00039: val_loss improved from 65048028676.09600 to 64214742892.54400, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 59780959221.9876 - val_loss: 63386770505.7280\n",
      "\n",
      "Epoch 00040: val_loss improved from 64214742892.54400 to 63386770505.72800, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 58964683500.2027 - val_loss: 62554961117.1840\n",
      "\n",
      "Epoch 00041: val_loss improved from 63386770505.72800 to 62554961117.18400, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 58138298537.3013 - val_loss: 61735717634.0480\n",
      "\n",
      "Epoch 00042: val_loss improved from 62554961117.18400 to 61735717634.04800, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 57323855722.7236 - val_loss: 60914490802.1760\n",
      "\n",
      "Epoch 00043: val_loss improved from 61735717634.04800 to 60914490802.17600, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 56503696352.1422 - val_loss: 60096960069.6320\n",
      "\n",
      "Epoch 00044: val_loss improved from 60914490802.17600 to 60096960069.63200, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 55668120533.2196 - val_loss: 59280067788.8000\n",
      "\n",
      "Epoch 00045: val_loss improved from 60096960069.63200 to 59280067788.80000, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 54842026473.6996 - val_loss: 58457415778.3040\n",
      "\n",
      "Epoch 00046: val_loss improved from 59280067788.80000 to 58457415778.30400, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 54010812614.4284 - val_loss: 57647501213.6960\n",
      "\n",
      "Epoch 00047: val_loss improved from 58457415778.30400 to 57647501213.69600, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 53177819831.4098 - val_loss: 56829615800.3200\n",
      "\n",
      "Epoch 00048: val_loss improved from 57647501213.69600 to 56829615800.32000, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 52360600900.9493 - val_loss: 56027553169.4080\n",
      "\n",
      "Epoch 00049: val_loss improved from 56829615800.32000 to 56027553169.40800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 51520948746.4676 - val_loss: 55215088926.7200\n",
      "\n",
      "Epoch 00050: val_loss improved from 56027553169.40800 to 55215088926.72000, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 50697004821.1627 - val_loss: 54416942628.8640\n",
      "\n",
      "Epoch 00051: val_loss improved from 55215088926.72000 to 54416942628.86400, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 49875405404.3876 - val_loss: 53624083316.7360\n",
      "\n",
      "Epoch 00052: val_loss improved from 54416942628.86400 to 53624083316.73600, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 49051193566.0942 - val_loss: 52843989532.6720\n",
      "\n",
      "Epoch 00053: val_loss improved from 53624083316.73600 to 52843989532.67200, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 48227908357.6889 - val_loss: 52055709745.1520\n",
      "\n",
      "Epoch 00054: val_loss improved from 52843989532.67200 to 52055709745.15200, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 47403340121.8844 - val_loss: 51286491725.8240\n",
      "\n",
      "Epoch 00055: val_loss improved from 52055709745.15200 to 51286491725.82400, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 46613951727.3885 - val_loss: 50534507249.6640\n",
      "\n",
      "Epoch 00056: val_loss improved from 51286491725.82400 to 50534507249.66400, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 45824541298.2329 - val_loss: 49813665021.9520\n",
      "\n",
      "Epoch 00057: val_loss improved from 50534507249.66400 to 49813665021.95200, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45071834655.4027 - val_loss: 49112425070.5920\n",
      "\n",
      "Epoch 00058: val_loss improved from 49813665021.95200 to 49112425070.59200, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44345352439.5804 - val_loss: 48432106635.2640\n",
      "\n",
      "Epoch 00059: val_loss improved from 49112425070.59200 to 48432106635.26400, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43661941178.3680 - val_loss: 47821221199.8720\n",
      "\n",
      "Epoch 00060: val_loss improved from 48432106635.26400 to 47821221199.87200, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43024287771.3067 - val_loss: 47246677344.2560\n",
      "\n",
      "Epoch 00061: val_loss improved from 47821221199.87200 to 47246677344.25600, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 42434092837.5467 - val_loss: 46741821325.3120\n",
      "\n",
      "Epoch 00062: val_loss improved from 47246677344.25600 to 46741821325.31200, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41892284414.1796 - val_loss: 46289688592.3840\n",
      "\n",
      "Epoch 00063: val_loss improved from 46741821325.31200 to 46289688592.38400, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41396230582.7271 - val_loss: 45897796452.3520\n",
      "\n",
      "Epoch 00064: val_loss improved from 46289688592.38400 to 45897796452.35200, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40957934294.3573 - val_loss: 45518593884.1600\n",
      "\n",
      "Epoch 00065: val_loss improved from 45897796452.35200 to 45518593884.16000, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40559064143.1893 - val_loss: 45203121700.8640\n",
      "\n",
      "Epoch 00066: val_loss improved from 45518593884.16000 to 45203121700.86400, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40185660833.7920 - val_loss: 44915859357.6960\n",
      "\n",
      "Epoch 00067: val_loss improved from 45203121700.86400 to 44915859357.69600, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39866041088.2276 - val_loss: 44656491364.3520\n",
      "\n",
      "Epoch 00068: val_loss improved from 44915859357.69600 to 44656491364.35200, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39548505059.7831 - val_loss: 44428516720.6400\n",
      "\n",
      "Epoch 00069: val_loss improved from 44656491364.35200 to 44428516720.64000, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 39269333139.4560 - val_loss: 44217486540.8000\n",
      "\n",
      "Epoch 00070: val_loss improved from 44428516720.64000 to 44217486540.80000, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 39018746580.5369 - val_loss: 44027694514.1760\n",
      "\n",
      "Epoch 00071: val_loss improved from 44217486540.80000 to 44027694514.17600, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 38775277986.7022 - val_loss: 43818001956.8640\n",
      "\n",
      "Epoch 00072: val_loss improved from 44027694514.17600 to 43818001956.86400, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38536078988.6293 - val_loss: 43646061936.6400\n",
      "\n",
      "Epoch 00073: val_loss improved from 43818001956.86400 to 43646061936.64000, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 38323034567.1111 - val_loss: 43454754029.5680\n",
      "\n",
      "Epoch 00074: val_loss improved from 43646061936.64000 to 43454754029.56800, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 38116566543.0187 - val_loss: 43281596088.3200\n",
      "\n",
      "Epoch 00075: val_loss improved from 43454754029.56800 to 43281596088.32000, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37925072677.5467 - val_loss: 43158784114.6880\n",
      "\n",
      "Epoch 00076: val_loss improved from 43281596088.32000 to 43158784114.68800, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37730638750.6062 - val_loss: 43000789401.6000\n",
      "\n",
      "Epoch 00077: val_loss improved from 43158784114.68800 to 43000789401.60000, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37545695324.8427 - val_loss: 42832945938.4320\n",
      "\n",
      "Epoch 00078: val_loss improved from 43000789401.60000 to 42832945938.43200, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37369545315.6693 - val_loss: 42689178435.5840\n",
      "\n",
      "Epoch 00079: val_loss improved from 42832945938.43200 to 42689178435.58400, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37213279251.1147 - val_loss: 42551691640.8320\n",
      "\n",
      "Epoch 00080: val_loss improved from 42689178435.58400 to 42551691640.83200, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37043436697.8276 - val_loss: 42367208062.9760\n",
      "\n",
      "Epoch 00081: val_loss improved from 42551691640.83200 to 42367208062.97600, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36883514381.6533 - val_loss: 42236606906.3680\n",
      "\n",
      "Epoch 00082: val_loss improved from 42367208062.97600 to 42236606906.36800, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 36728396828.2169 - val_loss: 42110056824.8320\n",
      "\n",
      "Epoch 00083: val_loss improved from 42236606906.36800 to 42110056824.83200, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 36585878292.2524 - val_loss: 41972454588.4160\n",
      "\n",
      "Epoch 00084: val_loss improved from 42110056824.83200 to 41972454588.41600, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36449368939.6338 - val_loss: 41817125093.3760\n",
      "\n",
      "Epoch 00085: val_loss improved from 41972454588.41600 to 41817125093.37600, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36307057457.3796 - val_loss: 41719629086.7200\n",
      "\n",
      "Epoch 00086: val_loss improved from 41817125093.37600 to 41719629086.72000, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36177432406.6987 - val_loss: 41587646824.4480\n",
      "\n",
      "Epoch 00087: val_loss improved from 41719629086.72000 to 41587646824.44800, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36051885424.6400 - val_loss: 41470138187.7760\n",
      "\n",
      "Epoch 00088: val_loss improved from 41587646824.44800 to 41470138187.77600, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35921624032.1422 - val_loss: 41352127283.2000\n",
      "\n",
      "Epoch 00089: val_loss improved from 41470138187.77600 to 41352127283.20000, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35803357944.9458 - val_loss: 41223586906.1120\n",
      "\n",
      "Epoch 00090: val_loss improved from 41352127283.20000 to 41223586906.11200, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35698335460.0107 - val_loss: 41139544326.1440\n",
      "\n",
      "Epoch 00091: val_loss improved from 41223586906.11200 to 41139544326.14400, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35580043815.5947 - val_loss: 40976475979.7760\n",
      "\n",
      "Epoch 00092: val_loss improved from 41139544326.14400 to 40976475979.77600, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35472793399.7511 - val_loss: 40878393982.9760\n",
      "\n",
      "Epoch 00093: val_loss improved from 40976475979.77600 to 40878393982.97600, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35391152721.4649 - val_loss: 40774588203.0080\n",
      "\n",
      "Epoch 00094: val_loss improved from 40878393982.97600 to 40774588203.00800, saving model to best_model.h5\n",
      "Epoch 95/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35303738239.6587 - val_loss: 40658617303.0400\n",
      "\n",
      "Epoch 00095: val_loss improved from 40774588203.00800 to 40658617303.04000, saving model to best_model.h5\n",
      "Epoch 96/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35182854244.1245 - val_loss: 40588689539.0720\n",
      "\n",
      "Epoch 00096: val_loss improved from 40658617303.04000 to 40588689539.07200, saving model to best_model.h5\n",
      "Epoch 97/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35105872498.2329 - val_loss: 40501942091.7760\n",
      "\n",
      "Epoch 00097: val_loss improved from 40588689539.07200 to 40501942091.77600, saving model to best_model.h5\n",
      "Epoch 98/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35005353741.8809 - val_loss: 40411219787.7760\n",
      "\n",
      "Epoch 00098: val_loss improved from 40501942091.77600 to 40411219787.77600, saving model to best_model.h5\n",
      "Epoch 99/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34931741385.6142 - val_loss: 40335295021.0560\n",
      "\n",
      "Epoch 00099: val_loss improved from 40411219787.77600 to 40335295021.05600, saving model to best_model.h5\n",
      "Epoch 100/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34849365062.0871 - val_loss: 40204030836.7360\n",
      "\n",
      "Epoch 00100: val_loss improved from 40335295021.05600 to 40204030836.73600, saving model to best_model.h5\n",
      "Epoch 101/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34763294721.8204 - val_loss: 40149693136.8960\n",
      "\n",
      "Epoch 00101: val_loss improved from 40204030836.73600 to 40149693136.89600, saving model to best_model.h5\n",
      "Epoch 102/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34689906591.5164 - val_loss: 40099296378.8800\n",
      "\n",
      "Epoch 00102: val_loss improved from 40149693136.89600 to 40099296378.88000, saving model to best_model.h5\n",
      "Epoch 103/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34619039079.5378 - val_loss: 40023661019.1360\n",
      "\n",
      "Epoch 00103: val_loss improved from 40099296378.88000 to 40023661019.13600, saving model to best_model.h5\n",
      "Epoch 104/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34541981688.7182 - val_loss: 39974522781.6960\n",
      "\n",
      "Epoch 00104: val_loss improved from 40023661019.13600 to 39974522781.69600, saving model to best_model.h5\n",
      "Epoch 105/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34475419067.2782 - val_loss: 39880520302.5920\n",
      "\n",
      "Epoch 00105: val_loss improved from 39974522781.69600 to 39880520302.59200, saving model to best_model.h5\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 12us/step - loss: 34409279844.8071 - val_loss: 39797475049.4720\n",
      "\n",
      "Epoch 00106: val_loss improved from 39880520302.59200 to 39797475049.47200, saving model to best_model.h5\n",
      "Epoch 107/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34347008436.9067 - val_loss: 39678428577.7920\n",
      "\n",
      "Epoch 00107: val_loss improved from 39797475049.47200 to 39678428577.79200, saving model to best_model.h5\n",
      "Epoch 108/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34287083181.3973 - val_loss: 39658844979.2000\n",
      "\n",
      "Epoch 00108: val_loss improved from 39678428577.79200 to 39658844979.20000, saving model to best_model.h5\n",
      "Epoch 109/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34224641645.6818 - val_loss: 39636056375.2960\n",
      "\n",
      "Epoch 00109: val_loss improved from 39658844979.20000 to 39636056375.29600, saving model to best_model.h5\n",
      "Epoch 110/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34163118793.6142 - val_loss: 39521863106.5600\n",
      "\n",
      "Epoch 00110: val_loss improved from 39636056375.29600 to 39521863106.56000, saving model to best_model.h5\n",
      "Epoch 111/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34101883917.6533 - val_loss: 39476215676.9280\n",
      "\n",
      "Epoch 00111: val_loss improved from 39521863106.56000 to 39476215676.92800, saving model to best_model.h5\n",
      "Epoch 112/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34054626374.0871 - val_loss: 39423951536.1280\n",
      "\n",
      "Epoch 00112: val_loss improved from 39476215676.92800 to 39423951536.12800, saving model to best_model.h5\n",
      "Epoch 113/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33996209076.4516 - val_loss: 39335197966.3360\n",
      "\n",
      "Epoch 00113: val_loss improved from 39423951536.12800 to 39335197966.33600, saving model to best_model.h5\n",
      "Epoch 114/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33945380321.5076 - val_loss: 39317181071.3600\n",
      "\n",
      "Epoch 00114: val_loss improved from 39335197966.33600 to 39317181071.36000, saving model to best_model.h5\n",
      "Epoch 115/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33895051136.5689 - val_loss: 39271611039.7440\n",
      "\n",
      "Epoch 00115: val_loss improved from 39317181071.36000 to 39271611039.74400, saving model to best_model.h5\n",
      "Epoch 116/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33847080067.0720 - val_loss: 39208574779.3920\n",
      "\n",
      "Epoch 00116: val_loss improved from 39271611039.74400 to 39208574779.39200, saving model to best_model.h5\n",
      "Epoch 117/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33808622234.2827 - val_loss: 39153998528.5120\n",
      "\n",
      "Epoch 00117: val_loss improved from 39208574779.39200 to 39153998528.51200, saving model to best_model.h5\n",
      "Epoch 118/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33750373105.6640 - val_loss: 39077553963.0080\n",
      "\n",
      "Epoch 00118: val_loss improved from 39153998528.51200 to 39077553963.00800, saving model to best_model.h5\n",
      "Epoch 119/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 33713382945.2231 - val_loss: 39070849368.0640\n",
      "\n",
      "Epoch 00119: val_loss improved from 39077553963.00800 to 39070849368.06400, saving model to best_model.h5\n",
      "Epoch 120/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33671119437.8240 - val_loss: 39037835870.2080\n",
      "\n",
      "Epoch 00120: val_loss improved from 39070849368.06400 to 39037835870.20800, saving model to best_model.h5\n",
      "Epoch 121/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33622327783.8791 - val_loss: 38955241504.7680\n",
      "\n",
      "Epoch 00121: val_loss improved from 39037835870.20800 to 38955241504.76800, saving model to best_model.h5\n",
      "Epoch 122/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33578996141.6249 - val_loss: 38900926939.1360\n",
      "\n",
      "Epoch 00122: val_loss improved from 38955241504.76800 to 38900926939.13600, saving model to best_model.h5\n",
      "Epoch 123/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33536964597.0773 - val_loss: 38876731932.6720\n",
      "\n",
      "Epoch 00123: val_loss improved from 38900926939.13600 to 38876731932.67200, saving model to best_model.h5\n",
      "Epoch 124/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33500475932.6720 - val_loss: 38860265717.7600\n",
      "\n",
      "Epoch 00124: val_loss improved from 38876731932.67200 to 38860265717.76000, saving model to best_model.h5\n",
      "Epoch 125/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33464907260.8142 - val_loss: 38826009624.5760\n",
      "\n",
      "Epoch 00125: val_loss improved from 38860265717.76000 to 38826009624.57600, saving model to best_model.h5\n",
      "Epoch 126/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33434713070.7058 - val_loss: 38759668842.4960\n",
      "\n",
      "Epoch 00126: val_loss improved from 38826009624.57600 to 38759668842.49600, saving model to best_model.h5\n",
      "Epoch 127/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33394509862.2293 - val_loss: 38714550321.1520\n",
      "\n",
      "Epoch 00127: val_loss improved from 38759668842.49600 to 38714550321.15200, saving model to best_model.h5\n",
      "Epoch 128/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33357455727.7298 - val_loss: 38705615634.4320\n",
      "\n",
      "Epoch 00128: val_loss improved from 38714550321.15200 to 38705615634.43200, saving model to best_model.h5\n",
      "Epoch 129/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33322727991.9787 - val_loss: 38642503909.3760\n",
      "\n",
      "Epoch 00129: val_loss improved from 38705615634.43200 to 38642503909.37600, saving model to best_model.h5\n",
      "Epoch 130/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33286661217.3938 - val_loss: 38597904498.6880\n",
      "\n",
      "Epoch 00130: val_loss improved from 38642503909.37600 to 38597904498.68800, saving model to best_model.h5\n",
      "Epoch 131/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33252904674.1902 - val_loss: 38588660449.2800\n",
      "\n",
      "Epoch 00131: val_loss improved from 38597904498.68800 to 38588660449.28000, saving model to best_model.h5\n",
      "Epoch 132/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33224918514.8018 - val_loss: 38529613234.1760\n",
      "\n",
      "Epoch 00132: val_loss improved from 38588660449.28000 to 38529613234.17600, saving model to best_model.h5\n",
      "Epoch 133/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33190210593.6782 - val_loss: 38498867806.2080\n",
      "\n",
      "Epoch 00133: val_loss improved from 38529613234.17600 to 38498867806.20800, saving model to best_model.h5\n",
      "Epoch 134/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33166816781.1982 - val_loss: 38446092615.6800\n",
      "\n",
      "Epoch 00134: val_loss improved from 38498867806.20800 to 38446092615.68000, saving model to best_model.h5\n",
      "Epoch 135/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33130609457.3796 - val_loss: 38429994188.8000\n",
      "\n",
      "Epoch 00135: val_loss improved from 38446092615.68000 to 38429994188.80000, saving model to best_model.h5\n",
      "Epoch 136/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33119490135.3813 - val_loss: 38455079927.8080\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 38429994188.80000\n",
      "Epoch 137/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 33075230091.9467 - val_loss: 38360250580.9920\n",
      "\n",
      "Epoch 00137: val_loss improved from 38429994188.80000 to 38360250580.99200, saving model to best_model.h5\n",
      "Epoch 138/400\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 33050356289.0809 - val_loss: 38310647627.7760\n",
      "\n",
      "Epoch 00138: val_loss improved from 38360250580.99200 to 38310647627.77600, saving model to best_model.h5\n",
      "Epoch 139/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33017948624.2133 - val_loss: 38306381004.8000\n",
      "\n",
      "Epoch 00139: val_loss improved from 38310647627.77600 to 38306381004.80000, saving model to best_model.h5\n",
      "Epoch 140/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32989488866.1902 - val_loss: 38296360091.6480\n",
      "\n",
      "Epoch 00140: val_loss improved from 38306381004.80000 to 38296360091.64800, saving model to best_model.h5\n",
      "Epoch 141/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32979157902.2222 - val_loss: 38283316002.8160\n",
      "\n",
      "Epoch 00141: val_loss improved from 38296360091.64800 to 38283316002.81600, saving model to best_model.h5\n",
      "Epoch 142/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32936257102.7342 - val_loss: 38207856377.8560\n",
      "\n",
      "Epoch 00142: val_loss improved from 38283316002.81600 to 38207856377.85600, saving model to best_model.h5\n",
      "Epoch 143/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32926904334.5636 - val_loss: 38199855087.6160\n",
      "\n",
      "Epoch 00143: val_loss improved from 38207856377.85600 to 38199855087.61600, saving model to best_model.h5\n",
      "Epoch 144/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32887741881.4578 - val_loss: 38198765125.6320\n",
      "\n",
      "Epoch 00144: val_loss improved from 38199855087.61600 to 38198765125.63200, saving model to best_model.h5\n",
      "Epoch 145/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32866151701.6178 - val_loss: 38168843452.4160\n",
      "\n",
      "Epoch 00145: val_loss improved from 38198765125.63200 to 38168843452.41600, saving model to best_model.h5\n",
      "Epoch 146/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32844073466.0836 - val_loss: 38104048435.2000\n",
      "\n",
      "Epoch 00146: val_loss improved from 38168843452.41600 to 38104048435.20000, saving model to best_model.h5\n",
      "Epoch 147/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32823448882.7449 - val_loss: 38084934664.1920\n",
      "\n",
      "Epoch 00147: val_loss improved from 38104048435.20000 to 38084934664.19200, saving model to best_model.h5\n",
      "Epoch 148/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32799449681.4649 - val_loss: 38072868339.7120\n",
      "\n",
      "Epoch 00148: val_loss improved from 38084934664.19200 to 38072868339.71200, saving model to best_model.h5\n",
      "Epoch 149/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32772458883.7547 - val_loss: 38065437343.7440\n",
      "\n",
      "Epoch 00149: val_loss improved from 38072868339.71200 to 38065437343.74400, saving model to best_model.h5\n",
      "Epoch 150/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32751987806.6631 - val_loss: 38008082759.6800\n",
      "\n",
      "Epoch 00150: val_loss improved from 38065437343.74400 to 38008082759.68000, saving model to best_model.h5\n",
      "Epoch 151/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32732362072.9742 - val_loss: 37986270773.2480\n",
      "\n",
      "Epoch 00151: val_loss improved from 38008082759.68000 to 37986270773.24800, saving model to best_model.h5\n",
      "Epoch 152/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32714497126.8551 - val_loss: 37978890764.2880\n",
      "\n",
      "Epoch 00152: val_loss improved from 37986270773.24800 to 37978890764.28800, saving model to best_model.h5\n",
      "Epoch 153/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32686127815.7938 - val_loss: 37992514650.1120\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 37978890764.28800\n",
      "Epoch 154/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32670563933.2978 - val_loss: 37929588064.2560\n",
      "\n",
      "Epoch 00154: val_loss improved from 37978890764.28800 to 37929588064.25600, saving model to best_model.h5\n",
      "Epoch 155/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32643100182.3004 - val_loss: 37923952721.9200\n",
      "\n",
      "Epoch 00155: val_loss improved from 37929588064.25600 to 37923952721.92000, saving model to best_model.h5\n",
      "Epoch 156/400\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 32639153899.2924 - val_loss: 37904727080.9600\n",
      "\n",
      "Epoch 00156: val_loss improved from 37923952721.92000 to 37904727080.96000, saving model to best_model.h5\n",
      "Epoch 157/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32613794944.3413 - val_loss: 37936861806.5920\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 37904727080.96000\n",
      "Epoch 158/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32586484540.3022 - val_loss: 37883475689.4720\n",
      "\n",
      "Epoch 00158: val_loss improved from 37904727080.96000 to 37883475689.47200, saving model to best_model.h5\n",
      "Epoch 159/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32579593554.6027 - val_loss: 37853698523.1360\n",
      "\n",
      "Epoch 00159: val_loss improved from 37883475689.47200 to 37853698523.13600, saving model to best_model.h5\n",
      "Epoch 160/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32548355480.6898 - val_loss: 37833799860.2240\n",
      "\n",
      "Epoch 00160: val_loss improved from 37853698523.13600 to 37833799860.22400, saving model to best_model.h5\n",
      "Epoch 161/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32538943182.1653 - val_loss: 37796682727.4240\n",
      "\n",
      "Epoch 00161: val_loss improved from 37833799860.22400 to 37796682727.42400, saving model to best_model.h5\n",
      "Epoch 162/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32507544910.0516 - val_loss: 37774902165.5040\n",
      "\n",
      "Epoch 00162: val_loss improved from 37796682727.42400 to 37774902165.50400, saving model to best_model.h5\n",
      "Epoch 163/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32493366933.7316 - val_loss: 37761643053.0560\n",
      "\n",
      "Epoch 00163: val_loss improved from 37774902165.50400 to 37761643053.05600, saving model to best_model.h5\n",
      "Epoch 164/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32475281315.1573 - val_loss: 37785489571.8400\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 37761643053.05600\n",
      "Epoch 165/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32457725282.0764 - val_loss: 37763269394.4320\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 37761643053.05600\n",
      "Model score: 0.6731525788821355\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers=[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
